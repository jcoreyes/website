<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Please delete this script if you use this HTML. -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-7580334-1');
  </script>
  <meta name="viewport" content="width=500">
  <link href="stylesheet.css" rel="stylesheet" type="text/css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>John D. Co-Reyes</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>John D. Co-Reyes</name>
              </p>
              <p>I am a PhD student at <a href="https://bair.berkeley.edu/">Berkeley AI Research Lab (BAIR)</a> working with <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, where I work on machine learning, deep reinforcement learning, and robotics.
              </p>
              <p>
                I did my undergrad at Caltech where I worked with <a href="http://www.yisongyue.com/en">Yisong Yue</a> and <a href="http://www.vision.caltech.edu/html-files/Perona.html">Pietro Perona</a> on computer vision.
              </p>
              <p align=center>
                <a href="mailto:jcoreyes@eecs.berkeley.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=xBH73TYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/john-co-reyes-b3202366"> LinkedIn </a>
                </a> &nbsp/&nbsp
                <a href="https://github.com/jcoreyes"> Github </a>
              </p>
            </td>
            <td width="33%">
              <img src="images/profile_circle4.png">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                My long-term goal is help build general AI and use that to accelerate research in other fields. I'm interested in combining deep learning with reinforcement learning in order to solve complex decision making problems. Areas of interest include computer vision, representation learning, and natural language processing all at the intersection with control and reinforcement learning in order to build complex autonomous agents that can understand the world, learn from previous behavior, and operate intelligently with humans. I'm currently interested in studying the emergence of intelligent behavior in complex environments with simple unsupervised learning objectives.
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

                  <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='porlight_image'>
                <img src='images/env_non_episodic_draft1.png'; style='height: 100%; width: 100%; object-fit: contain' ></div>
                <img src='images/env_non_episodic_draft1.png'; style='height: 100%; width: 100%; object-fit: contain'>
              </div>
              <script type="text/javascript">
                function porlight_start() {
                  document.getElementById('porlight_image').style.opacity = "1";
                }

                function porlight_stop() {
                  document.getElementById('porlight_image').style.opacity = "0";
                }
                porlight_stop()
              </script>
            </td>

            <td valign="middle" width="75%">
              <a href="">
                <papertitle>Ecological Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>John D. Co-Reyes*</strong>,
              <a href="http://suvan.sh/">Suvansh Sanjeev*</a>,
              <a href="https://people.eecs.berkeley.edu/~gberseth/">Glen Berseth</a>,
              <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta</a>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>
              <em>Deep Reinforcement Learning Workshop at NeurIPS</em>, 2019
              <br>
              <a href="https://sites.google.com/view/ecological-rl/">project page</a>
              <br>
              <p></p>
              <p>Reinforcement learning algorithms have been shown to effectively learn tasks in a variety of static, deterministic, and  simplistic environments, but their application to environments which are characteristic of dynamic lifelong settings encountered in the real world has been limited. Understanding the impact of specific environmental properties on the learning dynamics of reinforcement learning algorithms is important as we want to align the environments in which we develop our algorithms with the real world, and this is strongly coupled with the type of intelligence which can be learned. In this work, we study what we refer to as ecological reinforcement learning: the interaction between properties of the environment and the reinforcement learning agent. To this end, we introduce environments with characteristics that we argue better reflect natural environments: non-episodic learning, uninformative ``fundamental drive'' reward signals, and natural dynamics that cause the environment to change even when the agent fails to take intelligent actions. We show these factors can have a profound effect on the learning progress of reinforcement learning algorithms. Surprisingly, we find that these seemingly more challenging learning conditions can often make reinforcement learning agents learn more effectively. Through this study, we hope to shift the focus of the community towards learning in realistic, natural environments with dynamic elements..</p>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='porlight_image'>
                <img src='images/double-bridge-close-topfar_3.gif'; style='height: 100%; width: 100%; object-fit: contain' ></div>
                <img src='images/double-bridge-close-topfar_3.gif'; style='height: 100%; width: 100%; object-fit: contain'>
              </div>
              <script type="text/javascript">
                function porlight_start() {
                  document.getElementById('porlight_image').style.opacity = "1";
                }

                function porlight_stop() {
                  document.getElementById('porlight_image').style.opacity = "0";
                }
                porlight_stop()
              </script>
            </td>

            <td valign="middle" width="75%">
              <a href="https://arxiv.org/abs/1910.12827">
                <papertitle>Entity Abstraction in Visual Model-Based Reinforcement Learning</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/rishi-veerapaneni-308153133/">Rishi Veerapaneni*</a>,
              <strong>John D. Co-Reyes*</strong>,
              <a href="http://mbchang.github.io/">Michael Chang*</a>,
              <a href="https://people.eecs.berkeley.edu/~janner/">Michael Janner</a>,
              <a href="http://people.eecs.berkeley.edu/~cbfinn/">Chelsea Finn</a>,
              <a href="https://jiajunwu.com/">Jiajun Wu</a>,
              <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>
              <em>Conference on Robot Learning</em>, 2019
              <br>
              <a href="https://sites.google.com/view/op3website/">project page</a> /
              <a href="data/CoRL_2019_OP3_Poster.pdf">poster</a> 
              <br>
              <p></p>
              <p>We test the hypothesis that modeling a scene in terms of entities and their local interactions, as opposed to modeling the scene globally, provides a significant benefit in generalizing to physical tasks in a combinatorial space the learner has not encountered before. We present object-centric perception, prediction, and planning (OP3), which to the best of our knowledge is the first entity-centric dynamic latent variable framework for model-based reinforcement learning that acquires entity representations from raw visual observations without supervision and uses them to predict and plan. OP3 enforces entity-abstraction -- symmetric processing of each entity representation with the same locally-scoped function -- which enables it to scale to model different numbers and configurations of objects from those in training. Our approach to solving the key technical challenge of grounding these entity representations to actual objects in the environment is to frame this variable binding problem as an inference problem, and we developing an interactive inference algorithm that uses temporal continuity and interactive feedback to bind information about object properties to the entity variables. On block-stacking tasks, OP3 generalizes to novel block configurations and more objects than observed during training, outperforming an oracle model that assumes access to object supervision and achieving two to three times better accuracy than a state-of-the-art video prediction model.</p>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='porlight_image'>
                <img src='images/LGPL.png'; style='height: 100%; width: 100%; object-fit: contain' ></div>
                <img src='images/LGPL.png'; style='height: 100%; width: 100%; object-fit: contain'>
              </div>
              <script type="text/javascript">
                function porlight_start() {
                  document.getElementById('porlight_image').style.opacity = "1";
                }

                function porlight_stop() {
                  document.getElementById('porlight_image').style.opacity = "0";
                }
                porlight_stop()
              </script>
            </td>

            <td valign="middle" width="75%">
              <a href="https://arxiv.org/abs/1811.07882">
                <papertitle>Guiding Policies with Language via Meta-Learning</papertitle>
              </a>
              <br>
              <strong>John D. Co-Reyes</strong>,
              <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta</a>, 
              <a href="http://suvan.sh/">Suvansh Sanjeev</a>, 
              <a href="https://www.stat.berkeley.edu/~yugroup/people/Nicholas.html">Nick Altieri</a>,
              <a href="http://web.mit.edu/jda/www/">Jacob Andreas</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>
              <em>International Conference on Learning Representations</em>, 2019
              <br>
              <em>Best Paper at Meta-Learning Workshop at NeurIPS</em>, 2018
              <br>
              <a href="https://sites.google.com/view/lgpl">project page</a> /
              <a href="data/LanguageCorrections_Poster_Final.pdf">poster</a> /
              <a href="https://docs.google.com/presentation/d/1NPc_Hf0be2EHZvUr4BTBbBV4SCBnjHNeM0ls661eOY4/edit?usp=sharing">workshop slides</a>
              
              <br>
              <p></p>
              <p>Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.</p>
            </td>
          </tr>

                <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='porlight_image'>
                <img src='images/MPCControl.png'; style='height: 100%; width: 100%; object-fit: contain' ></div>
                <img src='images/MPCControl.png'; style='height: 100%; width: 100%; object-fit: contain'>
              </div>
              <script type="text/javascript">
                function porlight_start() {
                  document.getElementById('porlight_image').style.opacity = "1";
                }

                function porlight_stop() {
                  document.getElementById('porlight_image').style.opacity = "0";
                }
                porlight_stop()
              </script>
            </td>

            <td valign="middle" width="75%">
              <a href="https://arxiv.org/abs/1806.02813">
                <papertitle>Self-Consistent Trajectory Autoencoder: Hierararchical Reinforcement Learning with Trajectory Embeddings</papertitle>
              </a>
              <br>
              <strong>John D. Co-Reyes*</strong>,
              <a href="https://yuxuanliu.com/">YuXuan Liu*</a>,
              <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta*</a>,
              
              <a href="https://ben-eysenbach.github.io/">Benjamin Eysenbach</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>
              <em>International Conference on Machine Learning</em>, 2018
              <br>
              <a href="https://sites.google.com/view/sectar/home">project page</a> /
              <a href="https://github.com/wyndwarrior/Sectar">code</a> /
              <a href="data/SeCTAr_Poster.pdf">poster</a> /
              <a href="https://www.dropbox.com/s/2hlhsfuons5xybd/sectar_new.mp4?dl=0">video</a>
              
              <br>
              <p></p>
              <p>In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.</p>
            </td>
          </tr>

                        <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='porlight_image'>
                <img src='images/ex2_thumb.png'; style='height: 100%; width: 100%; object-fit: contain' ></div>
                <img src='images/ex2_thumb.png'; style='height: 100%; width: 100%; object-fit: contain'>
              </div>
              <script type="text/javascript">
                function porlight_start() {
                  document.getElementById('porlight_image').style.opacity = "1";
                }

                function porlight_stop() {
                  document.getElementById('porlight_image').style.opacity = "0";
                }
                porlight_stop()
              </script>
            </td>

            <td valign="middle" width="75%">
              <a href="https://arxiv.org/abs/1703.01260">
                <papertitle>EX<sup>2</sup>: Exploration with Exemplar Models for Deep Reinforcement Learning</papertitle>
              </a>
              <br>
              
              <a href="https://people.eecs.berkeley.edu/~justinjfu/">Justin Fu*</a>,
              <strong>John D. Co-Reyes*</strong>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>
              <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2017
              <br>
              <em>Spotlight Presentation. 4.69% acceptance rate (152/3240)</em>
              <br>
              <a href="https://sites.google.com/view/ex2exploration">project page</a> /
              <a href="https://github.com/jcoreyes/ex2">code</a> /
              <a href="data/EX2+Poster_Landscape.pdf">poster</a> /
              <a href="data/SectarOral.pdf">conference slides</a> 
              
              <br>
              <p></p>
              <p>Efficient exploration in high-dimensional environments remains a key challenge in reinforcement learning (RL). Deep reinforcement learning methods have demonstrated the ability to learn with highly general policy classes for complex tasks with high-dimensional inputs, such as raw images. However, many of the most effective exploration techniques rely on tabular representations, or on the ability to construct a generative model over states and actions. Both are exceptionally difficult when these inputs are complex and high dimensional. On the other hand, it is comparatively easy to build discriminative models on top of complex states such as images using standard deep neural networks. This paper introduces a novel approach, EX2, which approximates state visitation densities by training an ensemble of discriminators, and assigns reward bonuses to rarely visited states. We demonstrate that EX2 achieves comparable performance to the state-of-the-art methods on low-dimensional tasks, and its effectiveness scales into high-dimensional state spaces such as visual domains without hand-designing features or density models.</p>
            </td>
          </tr>

        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="https://edge.edx.org/courses/course-v1:BerkeleyX+CS188+2018_SP/f57d184a147b4e739b74b01cb73f3caa/">Graduate Student Instructor, CS188 Spring 2018</a>
              <br>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
        </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <br>
        <p align="right"><font size="2">
          <a href="https://people.eecs.berkeley.edu/~barron/">I like this website.</a>
          </font>
        </p>
        </td>
      </tr>
      </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
