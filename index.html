<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Please delete this script if you use this HTML. -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-7580334-1');
  </script>
  <meta name="viewport" content="width=500">
  <link href="stylesheet.css" rel="stylesheet" type="text/css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>John D. Co-Reyes</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>John D. Co-Reyes</name>
              </p>
              <p>I am a PhD student at UC Berkeley working with <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, where I work on machine learning and robotics.
              </p>
              <p>
                I did my undergrad at Caltech where I worked with <a href="http://www.yisongyue.com/en">Yisong Yue</a> and <a href="http://www.vision.caltech.edu/html-files/Perona.html">Pietro Perona</a>.
              </p>
              <p align=center>
                <a href="mailto:jcoreyes@eecs.berkeley.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=xBH73TYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/john-co-reyes-b3202366"> LinkedIn </a>
              </p>
            </td>
            <td width="33%">
              <img src="images/JonBarron_circle.jpg">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, optimization, image processing, and photography. Much of my research is about inferring the physical world (shape, depth, motion, paint, light, colors, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='porlight_image'>
                <img src='images/LGPL.png'; style='height: 100%; width: 100%; object-fit: contain' ></div>
                <img src='images/LGPL.png'; style='height: 100%; width: 100%; object-fit: contain'>
              </div>
              <script type="text/javascript">
                function porlight_start() {
                  document.getElementById('porlight_image').style.opacity = "1";
                }

                function porlight_stop() {
                  document.getElementById('porlight_image').style.opacity = "0";
                }
                porlight_stop()
              </script>
            </td>

            <td valign="middle" width="75%">
              <a href="https://arxiv.org/abs/1811.07882">
                <papertitle>Guiding Policies with Language via Meta-Learning</papertitle>
              </a>
              <br>
              <strong>John D. Co-Reyes</strong>,
              <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta</a>, Suvansh Sanjeev,
              <a href="https://www.stat.berkeley.edu/~yugroup/people/Nicholas.html">Nick Altieri</a>,
              <a href="http://web.mit.edu/jda/www/">Jacob Andreas</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>
              <em>International Conference on Learning Representations</em>, 2019
              <br>
              <em>Best Paper at NeurIPS Meta-Learning Workshop</em>, 2018
              <br>
              <a href="https://sites.google.com/view/lgpl">project page</a> /
              <a href="data/LanguageCorrections_Poster_Final.pdf">poster</a> /
              <a href="https://docs.google.com/presentation/d/1NPc_Hf0be2EHZvUr4BTBbBV4SCBnjHNeM0ls661eOY4/edit?usp=sharing">workshop slides</a>
              
              <br>
              <p></p>
              <p>Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.</p>
            </td>
          </tr>

                <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='porlight_image'>
                <img src='images/MPCControl.png'; style='height: 100%; width: 100%; object-fit: contain' ></div>
                <img src='images/MPCControl.png'; style='height: 100%; width: 100%; object-fit: contain'>
              </div>
              <script type="text/javascript">
                function porlight_start() {
                  document.getElementById('porlight_image').style.opacity = "1";
                }

                function porlight_stop() {
                  document.getElementById('porlight_image').style.opacity = "0";
                }
                porlight_stop()
              </script>
            </td>

            <td valign="middle" width="75%">
              <a href="https://arxiv.org/abs/1806.02813">
                <papertitle>Self-Consistent Trajectory Autoencoder: Hierararchical Reinforcement Learning with Trajectory Embeddings</papertitle>
              </a>
              <br>
              <strong>John D. Co-Reyes*</strong>,
              <a href="https://yuxuanliu.com/">YuXuan Liu*</a>,
              <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta*</a>,
              
              <a href="https://ben-eysenbach.github.io/">Benjamin Eysenbach</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>
              <em>International Conference on Machine Learning</em>, 2018
              <br>
              <a href="https://sites.google.com/view/sectar/home">project page</a> /
              <a href="https://github.com/wyndwarrior/Sectar">code</a> /
              <a href="data/SeCTAr_Poster.pdf">poster</a> /
              <a href="https://www.dropbox.com/s/2hlhsfuons5xybd/sectar_new.mp4?dl=0">video</a>
              
              <br>
              <p></p>
              <p>In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.</p>
            </td>
          </tr>

                        <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='porlight_image'>
                <img src='images/ex2_thumb.png'; style='height: 100%; width: 100%; object-fit: contain' ></div>
                <img src='images/ex2_thumb.png'; style='height: 100%; width: 100%; object-fit: contain'>
              </div>
              <script type="text/javascript">
                function porlight_start() {
                  document.getElementById('porlight_image').style.opacity = "1";
                }

                function porlight_stop() {
                  document.getElementById('porlight_image').style.opacity = "0";
                }
                porlight_stop()
              </script>
            </td>

            <td valign="middle" width="75%">
              <a href="https://arxiv.org/abs/1703.01260">
                <papertitle>EX<sup>2</sup>: Exploration with Exemplar Models for Deep Reinforcement Learning</papertitle>
              </a>
              <br>
              
              <a href="https://people.eecs.berkeley.edu/~justinjfu/">Justin Fu*</a>,
              <strong>John D. Co-Reyes*</strong>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>
              <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2017
              <br>
              <em>Spotlight Presentation. 4.69% acceptance rate (152/3240)</em>
              <br>
              <a href="https://sites.google.com/view/ex2exploration">project page</a> /
              <a href="https://github.com/jcoreyes/ex2">code</a> /
              <a href="data/EX2+Poster_Landscape.pdf">poster</a> 
              
              <br>
              <p></p>
              <p>Efficient exploration in high-dimensional environments remains a key challenge in reinforcement learning (RL). Deep reinforcement learning methods have demonstrated the ability to learn with highly general policy classes for complex tasks with high-dimensional inputs, such as raw images. However, many of the most effective exploration techniques rely on tabular representations, or on the ability to construct a generative model over states and actions. Both are exceptionally difficult when these inputs are complex and high dimensional. On the other hand, it is comparatively easy to build discriminative models on top of complex states such as images using standard deep neural networks. This paper introduces a novel approach, EX2, which approximates state visitation densities by training an ensemble of discriminators, and assigns reward bonuses to rarely visited states. We demonstrate that EX2 achieves comparable performance to the state-of-the-art methods on low-dimensional tasks, and its effectiveness scales into high-dimensional state spaces such as visual domains without hand-designing features or density models.</p>
            </td>
          </tr>

        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="https://edge.edx.org/courses/course-v1:BerkeleyX+CS188+2018_SP/f57d184a147b4e739b74b01cb73f3caa/">Graduate Student Instructor, CS188 Spring 2018</a>
              <br>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
        </table>

      </td>
    </tr>
  </table>
</body>

</html>
